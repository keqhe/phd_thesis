\section{Background and Motivation}
\label{background}
In this section, we first give a brief background of transport schemes for
 congestion management in datacenter environments. We discuss how current 
tenant level bandwidth allocation techniques are not sufficient for managing network 
congestion in public cloud networks. Then we motivate the benefits of moving transport 
layer congestion control logic into the vSwitch. 


\subsection{Datacenter Transport Schemes}

 Today's datacenters host many intensive applications like search, advertising, analytics
and retail that require high bandwidth and low latency. These stringent requirements can
only be met by effective management of congestion hot-spots in the network fabric.
Congestion is caused by many reasons such as imperfect load balancing~\cite{al2010hedera},
flow collisions, failures, or oversubscription and its occurence in datacenters today is 
not rare. For example, Google reported that in their datacenter fabric congestion-based 
drops were observed when the network utilization approached 25\%~\cite{singh2015jupiter}.
Other studies have shown high variance and substantial increase in the 99.9$^{th}$ percentile latency for round-trip times in today's datacenters~\cite{wang2010impact,mogul2015inferring}. Therefore, significant motivation exists to ensure datacenter fabrics reduce the 
impact of network congestion on bandwidth and latency sensitive applications.

TCP, specifically its congestion control algorithm, is widely known to significantly 
impact network performance. As a result, datacenter TCP performance has been widely
studied and many new protocols have been proposed~\cite{alizadeh2011data, stephens2014practical, wu2010ictcp, mittal2015timely, jose2015high}. Specifically, DCTCP~\cite{alizadeh2011data} achieves high throughput and low latency by adjusting a TCP sender's rate based on
the fraction of packets that experience congestion. In DCTCP, switches are configured to 
mark packets with an ECN bit when their queue lengths exceed a threshold. By 
proportionally adjusting the rate of the sender based on the fraction of ECN bits 
received, DCTCP can keep queue lengths low while allowing flows to maintain high 
throughput. It has also been shown to increase fairness and stability over other schemes.
~\eric{cite} 

Transport schemes such as DCTCP have proven to be very effective in managing congestion 
in datacenter networks. However, they rely on the assumption that we can enforce a common
datacenter specific TCP variant in all end hosts because all machines in these 
environments are under a single administrative control. Unfortunately, this assumption 
breaks down in public cloud where cloud providers have no explicit control on the TCP 
stack run by the customer VMs. This makes it very challenging for a cloud administrator 
to exercise any transport level flow control for managing congestion in their 
datacenters. 


\subsection{Tenant Level Bandwidth Allocation}
Public cloud administrators not only have to worry about controlling congestion in the 
presence of arbitrary traffic patterns from untrusted VMs, they also have to find ways to
isolate the performance of different tenants and applications. This is to ensure that one
tenant or application cannot take an unfair share of the common network resource. 
Transport layer schemes such as DCTCP can control network congestion but they do not 
provide fair bandwidth allocation among tenants. It's because TCP's congestion control 
attempts to achieve fair sharing of available bandwidth across all competing flows in a 
tenant agnostic manner. Therefore, a tenant with more concurrent flows ends up getting 
higher share of the network bandwidth than a tenant with fewer active flows. The 
situation is further worsened by UDP flows since they are not subjected to any transport 
level congestion control.

In order to provide a degree of performance isolation in the network, datacenter 
operators implement a variety of bandwidth allocation schemes. Simple static rate 
limiters found on many default public cloud images enforce an upper-bound on the 
bandwidth available to different classes of VMs. More complex performance isolation 
schemes investigate how to allocate network bandwidth in datacenter networks by either 
guarenteeing or proportionally allocating bandwidth for tenants~\cite{rodrigues2011gatekeeper,Ballani2011oktopus,jeyakumar2013eyeq,shieh2011sharing,Guo2010Secondnet,Popa2012Faircloud,Xie2012Proteus,Lam2012NetShare}. For example, EyeQ~\cite{jeyakumar2013eyeq} provides a
single dedicated switch abstraction for tenant VMs and handles bandwidth allocation at 
the edge with a work-conserving distributed bandwidth arbitration scheme. It enforces 
rate limits at senders based on feedback generated by the receivers. Similarly, Seawall 
(TODO.....).

These bandwidth allocation schemes attempt to provide tenant level performance isolation 
regardless of the tenant transport stack and protocol. Unfortunately, these schemes by 
themselves are not sufficient to relieve network of congestion. In fact, several such 
schemes explicitly rely on a congestion free network fabric for optimal bandwidth 
allocation. As an example, EyeQ operates at VM granularity and can benefit from a 
DCTCP-like congestion control operating independently on each 
flow. Similarly, Seawall (TODO ....). Moreover, control loops of several of the bandwidth
allocation schemes work at larger timescales than TCP and can range from the order to 
tens of milliseconds~\cite{shieh2011sharing, rodrigues2011gatekeeper} to 
seconds~\cite{Ballani2011oktopus}. Hence, bandwidth allocation schemes can not react to 
congestion in the same way as TCP.

To illustrate this point, consider the topology (TODO .. show an example of 
congestion with bandwidth allocation...)


\subsection{Bandwidth Allocation with Transport Control}
In this work, we claim that public datacenter administrators can not relinquish 
congestion control to tenant VM transport stacks and solely rely on bandwidth arbitration
to achieve low latency and high utilization in the network fabric. It is important for a 
cloud provider to control both congestion control and bandwidth allocation aspects of the 
problem. The first is important to manage congestion and queue build up in the network 
fabric and the second is important to enforce tenant level resource limits. We also 
argue that effective congestion management in the fabric requires per-flow level control 
knobs at RTT time scales that are only possible in the transport layer. Therefore, its 
important for cloud providers to be able to exercise a desired transport level congestion
control policy in the presence of unmanaged and untrusted transport stacks in customer 
VMs. 

~\acdc{} is the first work that advocates moving transport level congestion control logic
out of the VM TPC stacks and in to the hypervisor. A key design constraint of~\acdc{} is 
to make it modular in nature so it can co-exist with any bandwidth allocation scheme. 
We see the two aspects as complimentary and claim that an administrator should be able to
combine any congestion control policy (e.g. DCTCP) with any bandwidth allocation (e.g. 
EyeQ) scheme. This means that our scheme should work with a variety of bandwidth 
allocation schemes and their associated rate-limiters (and also in the absence of both). 
In order to achieve this goal,~\acdc{} satisfies a variety of constraints. First, it is 
computationally light-weight in order to minimize the overhead of its adoption. Second, 
it doesn't require any changes to VMs or network hardware so it can be deployed in 
current and future networks. Third, our scheme works in the absence of specific topology 
information and works over arbitrary topologies. Fourth, it does not require 
administrators to possess information about tenant traffic patterns, VM placement, or 
flow demands.


\subsection{Datacenter Congestion Control in vSwitch}
In this work, we do not propose a new congestion control algorithm, rather we investigate
if congestion control can be moved to the vSwitch. In a public datacenter environment 
where VMs may deploy non-optimized TCP stacks, implementing congestion control in the 
vSwitch can provide an element of control without cooperation from VMs. This is an 
important criteria in untrusted environments or simply in cases where tenants cannot 
update their TCP stack due to various constraints (such as not being able to update their
OS or a dependence on a certain library). 

Another implication of moving the congestion control algorithm within the virtual switch
is that it allows for a {\em unified} congestion control algorithm to be implemented 
throughout the datacenter. Ensuring all tenants have the same stack can help limit 
unfairness in the network. Unfairness arises when stacks are handled differently in the 
fabric or when conservative and aggressive stacks coexist. In~\cite{judd2015nsdi}, it is 
shown ECN and non-ECN flows do not exist gracefully on the same fabric because packets 
belonging to non-ECN flows encounter severe packet drops when their packets exceed queue 
thresholds. Ideally, tenants shouldn't suffer based on a such a simple configuration 
issue. Additionally, stacks with different congestion control algorithms may not 
gracefully coexist on the same fabric. For example, Figure~\ref{tput_fairness_coexistence} shows the performance of five different TCP flows over the topology in Figure~\ref{dumbbell_topology}. Each flow has a different congestion control algorithm, all of which are 
available in today's Linux distribution. In this case the more aggressive stacks like 
TCP Illinois and TCP HighSpeed always achieve higher bandwidth. A tenant with the same 
standing as another tenant in the datacenter should not be able to achieve higher 
bandwidth by simply altering their stack.

Another benefit of moving congestion control to the vSwitch is it allows for different 
congestion control algorithms to be used on a per-flow basis. Currently, all flows within
a VM are tied to the same congestion control algorithm. This severely limits flexibility 
and forces tenants to optimize the performance a certain set of flows. For example, a 
webserver may choose a TCP stack that optimizes WAN performance, instead of the back-end 
performance within the datacenter. As studies have shown that TCP can be optimized for 
datacenters~\cite{alizadeh2011data, stephens2014practical, wu2010ictcp, mittal2015timely, jose2015high}, WAN environments, and even ~\eric{one more example, wireless/60Ghz/free-space optics?}, selecting a per-flow TCP stack has the potential to enhance network 
performance. By moving congestion control to the vSwitch, administrators can assign a 
specific congestion control algorithm to each flow, optimizing the network performance of
its clients in a seamless manner.~\eric{Also add QoS-based CC stuff here, since we have 
something.}

In the next section, we show that congestion control functionality is easy to 
port. Note that while the entire TCP stack may seem complicated and prone to 
high-overhead, the congestion control aspect of TCP is relatively light-weight and easy 
to implement. Indeed, studies have shown that most overhead comes from TCP's buffer 
management~\cite{optimize-tcp-receive}. The actual congestion control implementations in 
Linux are modular: DCTCP's Linux congestion control resides in {\tt tcp\_dctcp.c} and is 
only about 350 lines of code. Given the simplicity of congestion control, it is 
relatively easy to move its functionality to another layer. In~\acdc{}, we don't modify 
VMs so congestion control is run both in the vSwitch and in the VM. However, because 
congestion control is light-weight, the penalty imposed for running it twice is low: our 
benchmarks in Section~\todo{XXX} show the computational overhead of our scheme is less 
than 2\%.
