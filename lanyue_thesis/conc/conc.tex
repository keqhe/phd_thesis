
\section{Conclusion}

In this dissertation, we present three research projects --- 1) an edge-based traffic 
load balancing system (i.e., Presto) for datacenter networks, 2) virtualized congestion 
control technique for multi-tenant clouds (~\acdc TCP) and 3) low latency software rate 
limiters for cloud networks. All of them leverage the flexibility and high 
programmability of software-defined network edge (i.e., end-host networking) to improve 
the performance of datacenter networks. Each of these three projects focuses on one 
crucial functionality in datacenter networks. In the following, we provide a short 
conclusion for each project in turn.


{\bf Presto: Edge-based Load Balancing for Fast Datacenter Networks.} 
Modern datacenter networks are built with multi-stage Clos networks. There are 
usually tens to hundreds of network paths between two servers in the same datacenter.
The state-of-the-art traffic load balancing uses flow-level schemes (e.g., ECMP and WCMP). 
However, flow-level load balancing schemes suffer from the elephant flow collision problem. 
Elephant flow collisions lead to reduced throughput and increased latency. 
We propose Presto to address this classic problem. Presto is an end-host-based traffic 
load balancing system. At the sender side, Presto uses virtual switch (OVS) to 
chunk elephant flows into flowcells (a flowcell consists of multiple consecutive 
TCP segments and its maximum size is 64KB) and spread the flowcells evenly over 
multiple network paths. At the receiver side, we design and implement improved Generic 
Receive Offload (GRO) functionality in Linux networking subsystem to mask packet reordering for TCP. 
Presto makes sure mice flows (smaller than or equal to 64KB in size) are not exposed to packet 
ordering issue. Note that in realistic datacenter networks, the overwhelming majority of the 
flows are mice flows. For elephant flows, Presto's improved GRO logic puts flowcells in 
order before they are pushed up to TCP. Presto eliminates the elephant flow collision 
problem and demonstrates that subflow-level traffic load balancing is possible and effective. 


{\bf ~\acdc{} TCP: Virtual Congestion Control Enforcement for Datacenter Networks.}
 In multi-tenant clouds, tenants manage their own Virtual Machines (VMs). VM TCP stacks' congestion 
control algorithms can be outdated, inefficient or even misconfigured. Those outdated, inefficient 
or even misconfigured VM TCP stacks can cause severe network congestion and throughput fairness issue. 
Network congestion and throughput unfairness affect the performance of the applications running in 
clouds (e.g., increased task completion time). To address this problem, we present~\acdc{} TCP, 
a virtual congestion control enforcement technique for datacenter networks. The key idea of~\acdc{} TCP 
is to implement an intended congestion control algorithm (i.e., DCTCP) in the virtual switch in the hypervisor 
and the congestion control decisions are enforced via modifying TCP header's~\rwnd{} field. 
Our experiment results show that enforcing an intended congestion control algorithm in 
the network virtualization layer can greatly reduce network latency and improve throughput fairness. 
Also,~\acdc{} TCP's CPU and memory overhead is negligible.  

{\bf Low Latency Software Rate Limiters for Cloud Networks.}
 Rate limiters are employed to provide bandwidth allocation feature in multi-tenant clouds. 
For example, in Google Cloud Platform, different kinds of VMs are allocated with 
different maximum bandwidths. However, we find that rate limiters can increase network latency 
by an order of magnitude or even higher. That is because traffic shaping 
(the underlying mechanism of rate limiters) maintains a queue to absorb bursty traffic 
and dequeues packets into the network based on preconfigured rate. 
Queueing latency in rate limiter queue inflates end-to-end network latency.
To solve this 
problem, we first extend ECN into rate limiter queues and apply DCTCP on the end-host. 
Though this straightforward scheme reduces network latency significantly, it can also 
lead to TCP throughput oscillation because of coarse-grained segment-level ECN marking 
and long congestion control loop latency. To address the shortcomings of the straightforward scheme, 
we present~\dem{}, which directly sets TCP ECE bit in reverse ACKs and~\spring{} which runs 
a queue-length-based congestion control algorithm and enforces congestion control decisions 
via modifying~\rwnd{} field in reverse ACKs. Both~\dem{} and~\spring{} enable low latency, 
high network saturation rate limiters.~\dem{} relies on ECN support while~\spring{} is 
generic and can handle both ECN flows and non-ECN flows.

The techniques and mechanisms proposed in this dissertation are original. 
We believe the research work presented in this dissertation will be valuable to 
the computer networking research community.

\section{Lessons Learned}
{\bf Darkness Is Before the Dawn.}
The first important thing I learned during my Ph.D. study is that sometimes 
we have to go through the darkness before the dawn. The Presto project is such an example. 
Initially, we only implemented the Presto sender side logic, but we found that even in a simple topology, 
network throughput could not reach 9.3Gbps. We later found that it was caused by packet reordering. 
Before starting the project, we thought modern TCP stacks should be smart enough to tolerate 
a certain amount of packet reordering. However, we found it was not the case for high speed networks. 
I did extensive experiments to figure out how to improve the performance. 
%Frankly speaking, at the time, I did not know whether I could find one solution or not. 
After exploring in the darkness for around 2 months, we finally designed the first version of the 
improved GRO logic to mark packet reordering for TCP. 
The story tells me sometimes doing research is just like 
walking in the dark, but we should not lose hope. We will be fine if we keep exploring in the right direction.

{\bf Simple Things Can Work Well.}
I also learned that simple things can work pretty well in practice, maybe this is 
especially true in system and networking research. For example, the DCTCP congestion control law is extremely 
simple but this work has a huge impact in datacenter networking research. I think 
all of techniques or mechanisms introduced in this thesis are simple, especially, AC/DC TCP, DEM and Spring. 
We find simple things can boost performance significantly. I like simple and elegant solutions a lot. 

{\bf Start Simple and Build Software Step by Step.}
We need to write code to validate our ideas. 
A key thing I learned is that we should start small and 
build our prototypes step by step. Once we finish a small step, we can check whether 
the code we wrote works or not. In this way, we can have a deep understanding of the code we wrote. 
Also we can find bugs or performance issues at the early stage. 
All the projects presented in this thesis involve some kernel level programming. 
The practice works pretty well for me.

\iffalse
{\bf Thinking Twice Before Starting a New Project.}
Choosing a project is as important as finishing it. Finding a promising project is not always easy. 
The lesson I learned is that we should think twice before starting a new project and once we start it, we  should 
try our best. An example is the AC/DC TCP project. 
Actually, at that time, we had a few topics we might want to explore. 
I spent around one month to think which topic was more promising. I actually built a sketch in my head and 
thought what were the major steps and major challenges in a each project. 
This helps reduce the risk of starting an ambitious project.
\fi

{\bf Old Ideas or Techniques Can Reborn in New Use Cases.}
TCP was born in the late 1980s. But we observe TCP can not meet our performance goals in datacenters networks. 
So new research opportunities come. ECN-based or latency-based congestion control are not really new neither. 
But because of datacenter networks, they come to play important roles now. 
I have observed this phenomenon a lot of times in the last 5 years in networking research. 
I think applying old ideas or techniques is not a problem. The key thing is how we 
apply them, how we can improve upon them and how we can leverage the 
innovations in hardware or software to make the old 
ideas become more effective in the new settings. 

\section{Future Work}
In the following, we list potential research topics in datacenter networking we may explore in the future.

{\bf Automatic Datacenter Network Topology Analysis.}
Datacenter network topology determines how scalable the network is,
how resilient it is to link or switch failures and how easily the network
can be incrementally deployed. Today's practice is that network architects need to
manually infer (usually based on experiences) many key characteristics related to
the candidate network topologies. So we lack a scientific and formal method to
evaluate different network topologies. Therefore, there is a need to build a network
topology analysis framework to help network architect analyze and compare
candidate network topologies. To compare different network topologies, we need to
set up metrics to quantify different network topologies. Our first goal is to
identify a set of metrics (e.g., cost, wiring complexity, bandwidth, reliability, routing convergence) that
can accurately quantify datacenter network topologies.  Also, we need to define a set of workloads
and traffic patterns to run against the network. 
Next, we want to investigate whether we can design and implement
an automatic topology analysis framework to gain more insights and help design better network topologies.

{\bf Applying Machine Learning Techniques to Traffic Load Balancing.}
Datacenter networks are shared by a large amount of applications hosted in clouds. 
Based on the fact that traffic is multiplexed and the conjecture that the majority of traffic 
should be used by some top applications, predictable traffic patterns may 
exist in the datacenter networks. First, we want to measure traffic load on each link 
in a real datacenter network for a long time (e.g., a few months). Based on the measurement data, 
we analyze whether traffic loads can be predictable or not. If so, we can utilize 
big-data systems and machine learning techniques to help us apply better traffic load 
balancing schemes to reduce the possibility of network congestion and improve the 
performance of applications. We believe such a machine-learning-aided load balancing 
system can be applied to different scenarios. For example, it can be applied in 
both intra-datacenter networks and wide-area inter-datacenter networks.

{\bf Near Real Time Network Congestion Monitoring.}
Network congestion is a crucial performance hurdle for high performance cloud computing services like 
search, query and remote procedure calls.
Studies have shown that end-to-end network latency (TCP RTT) can be increased by tens of milliseconds due to network congestion. Such a huge network latency can affect customer's experience and can have significant negative impacts on revenue.
Therefore, one research question is whether we can monitor network congestion in (near) real time manner.
If the answer is yes, how we should provide such information to network administrators or developers
in the cloud? How we can quickly reroute network traffic to bypass congested network paths?
This research problem is challenging because network congestion information should be obtained in
(near) real time manner such that application traffic can be rerouted to avoid buffer building up.
In today's data center networks, the base line end-to-end latency is around 40 to
200 microseconds, so we need to reduce the ``monitoring and action''
control loop latency as small as possible.
To achieve this goal, we may need to explore recent advances in fast software packet processing 
(e.g., DPDK~\cite{intel-dpdk}) and modern hardware features in the switches.
%\cite{example}
