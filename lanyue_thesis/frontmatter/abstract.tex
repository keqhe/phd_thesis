
\svnidlong{$LastChangedBy$}{$LastChangedRevision$}{$LastChangedDate$}{$HeadURL: http://freevariable.com/dissertation/branches/diss-template/frontmatter/abstract.tex $}
\vcinfo{}

% Don't change anything above this line. It must stay in exactly the
% same spacing.


Datacenter networks are critical building blocks for modern cloud computing 
infrastructures. The capability of a datacenter network affects the performance of 
applications and services running in datacenters. Today's applications are 
becoming more and more demanding, requiring low latency, high throughput and low packet 
loss rate. 
Thus improving datacenter network performance is an important and timely research direction.

Datacenter networks are complicated systems with many functionalities and components. 
Any less optimized functionality can degrade network performance and affect applications.
In this dissertation, we discuss how we can leverage the flexibility and high 
programmability of software-defined network edge (i.e., end-host networking) to 
improve traffic load balancing, congestion control and rate limiting functionalities in datacenter networks.

We first look into traffic load balancing functionality in datacenter networks.
Modern datacenter networks need to deal with a variety of workloads, ranging
from latency-sensitive small flows to bandwidth-hungry
large flows. Load balancing schemes based on flow hashing,
e.g., ECMP, cause congestion when hash collisions occur
and can perform poorly in asymmetric topologies. Recent
proposals to load balance the network require centralized
traffic engineering, multipath-aware transport, or expensive
specialized hardware. We propose a mechanism that
avoids these limitations by (i) pushing load-balancing functionality
into the soft network edge (e.g., virtual switches)
such that no changes are required in the transport layer, customer
VMs, or networking hardware, and (ii) load balancing
on fine-grained, near-uniform units of data (flowcells)
that fit within end-host segment offload optimizations used
to support fast networking speeds. We design and implement
such a soft-edge load balancing scheme called Presto, and
evaluate it on a 10 Gbps physical testbed. We demonstrate
the computational impact of packet reordering on receivers
and propose a mechanism to handle reordering in the TCP
receive offload functionality. Presto's performance closely
tracks that of a single, non-blocking switch over many workloads
and is adaptive to failures and topology asymmetry.

Optimized traffic load balancing alone is not sufficient to 
guarantee high-performance datacenter networks. 
%In multi-tenant datacenters, tenants can
%seamlessly port their applications and services to the cloud.
Virtual Machine (VM) technology plays an integral role in
modern multi-tenant clouds by enabling a diverse set of software to be run
on a unified underlying framework. This flexibility, however,
comes at the cost of dealing with outdated, inefficient,
or misconfigured TCP stacks implemented in the VMs. We investigate if cloud providers can take control of a VM's
TCP congestion control algorithm without making changes
to the VM or network hardware. We propose a network edge-based 
 congestion control virtualization technique called AC/DC TCP.
AC/DC TCP exerts fine-grained control over arbitrary tenant
TCP stacks by enforcing per-flow congestion control in
the virtual switch (vSwitch) in the hypervisor. AC/DC TCP is light-weight,
flexible, scalable and can police non-conforming flows. Our experiment results demonstrate that 
implementing an administrator-defined congestion control algorithm in the
vSwitch (i.e., DCTCP) closely tracks its native performance,
regardless of the VM's TCP stack.

Presto and AC/DC TCP help reduce queueing latency in 
network switches (i.e., ``in network'' latency), but we observe that 
rate limiters on end-hosts can also increase network latency by an order of magnitude or even more. 
Rate limiters are employed to provide bandwidth allocation functionality, 
which is an indispensable feature of multi-tenant clouds. Rate limiters
maintain a queue of outstanding packets and control the speed at which
packets are dequeued into the network. This queuing introduces
additional network latency. For example, in our experiments, we find
that software rate limiting (HTB) increases latency by 1-3 milliseconds
across a range of different environment settings. 
We find that simply extending ECN into rate limiters is not sufficient. 
To this end, we propose two techniques \textemdash\xspace~\dem{} and~\spring{} to 
improve the performance of rate limiters.
Our experiment results demonstrate that~\dem{} and~\spring{}-enabled
rate limiters can achieve high stable throughput and low latency.





